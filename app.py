"""
API REST pour le scraping du calendrier √©conomique investing.com et des pronostics sportifs
Utilise FastAPI pour cr√©er une API REST asynchrone
"""
import logging
from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel, ValidationError
from typing import Optional, List, Dict, Any
from scrapers.investing_scraper import scrape_economic_calendar
from scrapers.pronostic import scrape_footyaccumulators, scrape_freesupertips, scrape_assopoker

# Importer le module d'unification
from unification import unification_router, init_postgres, load_initial_mappings

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cr√©er l'application FastAPI
app = FastAPI(
    title="MyScrapers API",
    description="API REST pour scraper le calendrier √©conomique d'investing.com et les pronostics sportifs",
    version="1.2.0"
)

# Monter le router d'unification
app.include_router(unification_router, prefix="/unify", tags=["Unification"])


# ============ √âv√©nements de startup/shutdown ============

@app.on_event("startup")
async def startup_event():
    """Initialiser PostgreSQL et charger les mappings au d√©marrage"""
    try:
        logger.info("üöÄ Initializing Unification Service...")

        # Initialiser PostgreSQL
        init_postgres()

        # Charger les mappings de base si PostgreSQL est vide
        await load_initial_mappings()

        logger.info("‚úÖ Unification service initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Error initializing unification service: {e}")
        logger.warning("‚ö†Ô∏è  Unification endpoints will not work properly")


class InvestingEvent(BaseModel):
    """Mod√®le pour un √©v√©nement √©conomique"""
    time: str = ""
    datetime: Optional[str] = None
    parsed_datetime: Optional[str] = None
    day: Optional[str] = None
    country: str = ""
    country_code: Optional[str] = None
    event: str = ""
    event_url: Optional[str] = None
    actual: str = ""
    forecast: str = ""
    previous: str = ""
    impact: str = ""
    event_id: Optional[str] = None


class InvestingScrapeRequest(BaseModel):
    """Mod√®le de requ√™te pour le scraping investing.com"""
    date_from: Optional[str] = None
    date_to: Optional[str] = None
    countries: Optional[List[int]] = None
    categories: Optional[List[str]] = None
    importance: Optional[List[int]] = None
    timezone: Optional[int] = 55
    time_filter: Optional[str] = "timeOnly"


class InvestingHoliday(BaseModel):
    """Mod√®le pour un jour f√©ri√©"""
    type: str = "holiday"
    time: str = ""
    day: Optional[str] = None
    country: str = ""
    event: str = ""
    impact: str = "Holiday"


class InvestingScrapeResponse(BaseModel):
    """Mod√®le de r√©ponse pour le scraping investing.com"""
    success: bool
    events: List[InvestingEvent]
    holidays: List[InvestingHoliday]
    date_range: dict
    total_events: int
    total_holidays: int
    error_message: Optional[str] = None


@app.get("/")
async def root():
    """Endpoint racine avec informations sur l'API"""
    return {
        "message": "MyScrapers API - Economic Calendar & Sports Betting Tips",
        "version": "1.2.0",
        "endpoints": {
            "GET /scrape/investing": "Scraper le calendrier √©conomique investing.com (GET)",
            "POST /scrape/investing": "Scraper le calendrier √©conomique investing.com (POST)",
            "GET /scrape/footyaccumulators": "Scraper les pronostics FootyAccumulators",
            "GET /scrape/freesupertips": "Scraper les pronostics FreeSupertips",
            "GET /scrape/assopoker": "Scraper les pronostics AssoPoker",
            "GET /health": "V√©rifier l'√©tat de l'API",
            "GET /unify/health": "V√©rifier l'√©tat du service d'unification",
            "POST /unify": "Unifier un sport ou type de pari",
            "POST /unify/bulk": "Unifier en batch (pour N8N)",
            "POST /unify/mapping/add": "Ajouter un mapping manuel",
            "POST /unify/mapping/bulk-add": "Ajouter plusieurs mappings",
            "GET /unify/mappings/{type}": "R√©cup√©rer tous les mappings (sport ou tip_type)",
            "GET /docs": "Documentation interactive (Swagger UI)"
        }
    }


@app.get("/health")
async def health():
    """Endpoint de sant√© pour v√©rifier que l'API fonctionne"""
    return {"status": "healthy", "service": "MyScrapers API"}


@app.get("/scrape/investing", response_model=InvestingScrapeResponse)
async def scrape_investing_get(
    date_from: Optional[str] = Query(None, description="Date de d√©but (YYYY-MM-DD)"),
    date_to: Optional[str] = Query(None, description="Date de fin (YYYY-MM-DD)"),
    timezone: Optional[int] = Query(55, description="ID du fuseau horaire"),
    time_filter: Optional[str] = Query("timeOnly", description="Filtre temporel")
):
    """
    Scraper le calendrier √©conomique d'investing.com via GET
    
    Args:
        date_from: Date de d√©but au format YYYY-MM-DD
        date_to: Date de fin au format YYYY-MM-DD
        timezone: ID du fuseau horaire (d√©faut: 55 pour UTC)
        time_filter: Filtre temporel (d√©faut: timeOnly)
    
    Returns:
        InvestingScrapeResponse avec les √©v√©nements √©conomiques
    """
    try:
        result = await scrape_economic_calendar(
            date_from=date_from,
            date_to=date_to,
            timezone=timezone,
            time_filter=time_filter
        )
        
        logger.info(f"Scraping result: success={result.get('success')}, total_events={result.get('total_events', 0)}")
        
        if result["success"]:
            # S√©parer les √©v√©nements √©conomiques des jours f√©ri√©s
            events = []
            holidays = []

            for event in result["events"]:
                try:
                    # V√©rifier si c'est un jour f√©ri√©
                    if event.get("type") == "holiday" or event.get("impact") == "Holiday":
                        # C'est un jour f√©ri√©
                        holiday_data = {
                            "type": "holiday",
                            "time": event.get("time", ""),
                            "day": event.get("day"),
                            "country": event.get("country", ""),
                            "event": event.get("event", ""),
                            "impact": "Holiday"
                        }
                        holidays.append(InvestingHoliday(**holiday_data))
                    else:
                        # C'est un √©v√©nement √©conomique
                        event_data = {
                            "time": event.get("time", ""),
                            "datetime": event.get("datetime"),
                            "parsed_datetime": event.get("parsed_datetime"),
                            "day": event.get("day"),
                            "country": event.get("country", ""),
                            "country_code": event.get("country_code"),
                            "event": event.get("event", ""),
                            "event_url": event.get("event_url"),
                            "actual": event.get("actual", ""),
                            "forecast": event.get("forecast", ""),
                            "previous": event.get("previous", ""),
                            "impact": event.get("impact", ""),
                            "event_id": event.get("event_id")
                        }
                        events.append(InvestingEvent(**event_data))
                except ValidationError as e:
                    logger.error(f"Erreur de validation pour l'√©v√©nement: {event}, erreur: {e}")
                    # Continuer avec les autres √©v√©nements m√™me si un √©choue
                    continue
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion de l'√©v√©nement: {event}, erreur: {str(e)}")
                    continue

            if not events and not holidays:
                raise HTTPException(
                    status_code=400,
                    detail="Aucun √©v√©nement ou jour f√©ri√© valide n'a pu √™tre converti"
                )

            return InvestingScrapeResponse(
                success=True,
                events=events,
                holidays=holidays,
                date_range=result["date_range"],
                total_events=len(events),
                total_holidays=len(holidays),
                error_message=None
            )
        else:
            error_msg = result.get('error_message', 'Erreur inconnue')
            logger.error(f"Scraping √©chou√©: {error_msg}")
            raise HTTPException(
                status_code=400,
                detail=f"Erreur lors du scraping: {error_msg}"
            )
    except HTTPException:
        raise
    except ValidationError as e:
        logger.error(f"Erreur de validation Pydantic: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Erreur de validation des donn√©es: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Erreur serveur: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur serveur: {str(e)}"
        )


@app.post("/scrape/investing", response_model=InvestingScrapeResponse)
async def scrape_investing_post(request: InvestingScrapeRequest):
    """
    Scraper le calendrier √©conomique d'investing.com via POST
    
    Args:
        request: InvestingScrapeRequest contenant les filtres de scraping
    
    Returns:
        InvestingScrapeResponse avec les √©v√©nements √©conomiques
    """
    try:
        result = await scrape_economic_calendar(
            date_from=request.date_from,
            date_to=request.date_to,
            countries=request.countries,
            categories=request.categories,
            importance=request.importance,
            timezone=request.timezone,
            time_filter=request.time_filter
        )
        
        if result["success"]:
            # S√©parer les √©v√©nements √©conomiques des jours f√©ri√©s
            events = []
            holidays = []

            for event in result["events"]:
                try:
                    # V√©rifier si c'est un jour f√©ri√©
                    if event.get("type") == "holiday" or event.get("impact") == "Holiday":
                        # C'est un jour f√©ri√©
                        holiday_data = {
                            "type": "holiday",
                            "time": event.get("time", ""),
                            "day": event.get("day"),
                            "country": event.get("country", ""),
                            "event": event.get("event", ""),
                            "impact": "Holiday"
                        }
                        holidays.append(InvestingHoliday(**holiday_data))
                    else:
                        # C'est un √©v√©nement √©conomique
                        events.append(InvestingEvent(**event))
                except ValidationError as e:
                    logger.error(f"Erreur de validation pour l'√©v√©nement: {event}, erreur: {e}")
                    continue
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion de l'√©v√©nement: {event}, erreur: {str(e)}")
                    continue

            return InvestingScrapeResponse(
                success=True,
                events=events,
                holidays=holidays,
                date_range=result["date_range"],
                total_events=len(events),
                total_holidays=len(holidays),
                error_message=None
            )
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Erreur lors du scraping: {result.get('error_message', 'Erreur inconnue')}"
            )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur serveur: {str(e)}"
        )


# =============================================================================
# ENDPOINTS POUR PRONOSTICS SPORTIFS
# =============================================================================

class PronosticTip(BaseModel):
    """Mod√®le pour un pronostic sportif simplifi√©"""
    match: Optional[str] = None
    dateTime: Optional[str] = None
    competition: Optional[str] = None
    sport: Optional[str] = None
    homeTeam: Optional[str] = None
    awayTeam: Optional[str] = None
    tipTitle: str
    tipType: Optional[str] = None
    tipText: Optional[str] = None
    reasonTip: Optional[str] = None
    odds: Optional[float] = None
    confidence: Optional[str] = None

    model_config = {
        "extra": "forbid",
        "json_schema_extra": {
            "example": {
                "match": "Arsenal vs Chelsea",
                "reasonTip": "Arsenal have won their last 3 matches..."
            }
        }
    }


class PronosticResponse(BaseModel):
    """Mod√®le de r√©ponse pour les pronostics sportifs"""
    success: bool
    pronostics: List[PronosticTip]
    total_pronostics: int
    error_message: Optional[str] = None


@app.get("/scrape/footyaccumulators")
async def scrape_footyaccumulators_endpoint():
    """
    Scraper les pronostics de FootyAccumulators

    Returns:
        PronosticResponse avec la liste des pronostics
    """
    try:
        result = await scrape_footyaccumulators(
            max_tips=None,
            debug_mode=False
        )

        logger.info(f"FootyAccumulators scraping: success={result.get('success')}, total={result.get('total_pronostics', 0)}")

        if result["success"]:
            # Retourner directement le dictionnaire pour √©viter la s√©rialisation Pydantic
            return JSONResponse(content={
                "success": True,
                "pronostics": result["pronostics"],
                "total_pronostics": result["total_pronostics"],
                "error_message": None
            })
        else:
            error_msg = result.get('error_message', 'Erreur inconnue')
            logger.error(f"FootyAccumulators scraping √©chou√©: {error_msg}")
            raise HTTPException(
                status_code=400,
                detail=f"Erreur lors du scraping: {error_msg}"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur serveur FootyAccumulators: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur serveur: {str(e)}"
        )


@app.get("/scrape/freesupertips")
async def scrape_freesupertips_endpoint():
    """
    Scraper les pronostics de FreeSupertips

    Returns:
        PronosticResponse avec la liste des pronostics
    """
    try:
        result = await scrape_freesupertips(
            max_tips=None,
            debug_mode=False
        )

        logger.info(f"FreeSupertips scraping: success={result.get('success')}, total={result.get('total_pronostics', 0)}")

        if result["success"]:
            # Retourner directement le dictionnaire pour √©viter la s√©rialisation Pydantic
            return JSONResponse(content={
                "success": True,
                "pronostics": result["pronostics"],
                "total_pronostics": result["total_pronostics"],
                "error_message": None
            })
        else:
            error_msg = result.get('error_message', 'Erreur inconnue')
            logger.error(f"FreeSupertips scraping √©chou√©: {error_msg}")
            raise HTTPException(
                status_code=400,
                detail=f"Erreur lors du scraping: {error_msg}"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur serveur FreeSupertips: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur serveur: {str(e)}"
        )


@app.get("/scrape/assopoker")
async def scrape_assopoker_endpoint():
    """
    Scraper les pronostics de AssoPoker

    Returns:
        PronosticResponse avec la liste des pronostics
    """
    try:
        result = await scrape_assopoker(
            max_tips=None,
            debug_mode=False
        )

        logger.info(f"AssoPoker scraping: success={result.get('success')}, total={result.get('total_pronostics', 0)}")

        if result["success"]:
            # Retourner directement le dictionnaire pour √©viter la s√©rialisation Pydantic
            return JSONResponse(content={
                "success": True,
                "pronostics": result["pronostics"],
                "total_pronostics": result["total_pronostics"],
                "error_message": None
            })
        else:
            error_msg = result.get('error_message', 'Erreur inconnue')
            logger.error(f"AssoPoker scraping √©chou√©: {error_msg}")
            raise HTTPException(
                status_code=400,
                detail=f"Erreur lors du scraping: {error_msg}"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur serveur AssoPoker: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur serveur: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)

