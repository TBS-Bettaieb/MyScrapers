# üéØ Service d'Unification - Sports & Tip Types

Service d'unification utilisant **Ollama + ChromaDB** pour normaliser les sports et types de paris provenant de diff√©rentes sources.

## üöÄ Quick Start

### Pr√©requis
- Python 3.11+
- Ollama install√© ([ollama.com](https://ollama.com))
- Docker (optionnel, pour d√©ploiement facile)

---

## üì¶ Installation

### Option 1 : Local (sans Docker)

```bash
# 1. Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. T√©l√©charger le mod√®le d'embeddings
ollama pull nomic-embed-text

# 3. Installer les d√©pendances Python
cd unification_service
pip install -r requirements.txt

# 4. Lancer le service
python main.py

# 5. Initialiser les mappings (dans un autre terminal)
python init_mappings.py
```

Le service sera disponible sur : **http://localhost:8002**

---

### Option 2 : Docker (recommand√©)

```bash
# 1. Construire et lancer
cd unification_service
docker-compose up -d

# 2. Attendre 30s que Ollama t√©l√©charge le mod√®le
docker-compose logs -f

# 3. Initialiser les mappings
python init_mappings.py
```

---

## üéØ API Endpoints

### 1. Health Check
```bash
GET http://localhost:8002/health
```

**R√©ponse :**
```json
{
  "status": "healthy",
  "ollama": "ok",
  "chromadb": "ok",
  "stats": {
    "sports_mappings": 15,
    "tip_types_mappings": 45
  }
}
```

---

### 2. Unifier un √©l√©ment
```bash
POST http://localhost:8002/unify
Content-Type: application/json

{
  "text": "calcio",
  "type": "sport",
  "threshold": 0.7
}
```

**R√©ponse :**
```json
{
  "original": "calcio",
  "unified": "football",
  "confidence": 0.95,
  "needs_review": false
}
```

---

### 3. Unifier en batch (pour N8N)
```bash
POST http://localhost:8002/unify/bulk
Content-Type: application/json

{
  "items": [
    {"sport": "calcio", "tipText": "1X2: 1"},
    {"sport": "soccer", "tipText": "BTTS"}
  ],
  "threshold": 0.7
}
```

**R√©ponse :**
```json
{
  "success": true,
  "total": 2,
  "items": [
    {
      "sport": "calcio",
      "tipText": "1X2: 1",
      "sport_unified": "football",
      "sport_confidence": 0.95,
      "sport_needs_review": false,
      "tipText_unified": "home_win",
      "tipText_confidence": 0.92,
      "tipText_needs_review": false
    },
    {
      "sport": "soccer",
      "tipText": "BTTS",
      "sport_unified": "football",
      "sport_confidence": 0.98,
      "sport_needs_review": false,
      "tipText_unified": "both_teams_score",
      "tipText_confidence": 0.91,
      "tipText_needs_review": false
    }
  ]
}
```

---

### 4. Ajouter un mapping
```bash
POST http://localhost:8002/mapping/add
Content-Type: application/json

{
  "original": "f√∫tbol",
  "unified": "football",
  "type": "sport"
}
```

---

### 5. R√©cup√©rer tous les mappings
```bash
GET http://localhost:8002/mappings/sport
GET http://localhost:8002/mappings/tip_type
```

---

## üîß Int√©gration N8N

### Workflow N8N complet

```
1. Webhook Trigger (recevoir les pronostics)
    ‚Üì
2. HTTP Request ‚Üí Unification Service (POST /unify/bulk)
    ‚Üì
3. Code Node (filtrer needs_review = true)
    ‚Üì
4. Switch Node
    ‚îú‚îÄ needs_review = false ‚Üí Continuer le workflow
    ‚îî‚îÄ needs_review = true ‚Üí Envoyer vers Airtable pour validation
    ‚Üì
5. Postgres (sauvegarder les pronostics unifi√©s)
```

### Configuration HTTP Request Node dans N8N

**URL :** `http://localhost:8002/unify/bulk`
**Method :** POST
**Body :**
```json
{
  "items": {{ $json.pronostics }},
  "threshold": 0.7
}
```

**Authentication :** None

---

## üìä Alimenter la base progressivement

### M√©thode 1 : Via l'API

```python
import requests

# Ajouter un nouveau mapping
requests.post("http://localhost:8002/mapping/add", json={
    "original": "basket-ball",
    "unified": "basketball",
    "type": "sport"
})
```

### M√©thode 2 : Via fichier JSON

Cr√©er `new_mappings.json` :
```json
[
  {"original": "hockey sur glace", "unified": "hockey", "type": "sport"},
  {"original": "hand", "unified": "handball", "type": "sport"}
]
```

Script Python :
```python
import requests
import json

with open("new_mappings.json") as f:
    mappings = json.load(f)

requests.post("http://localhost:8002/mapping/bulk-add", json=mappings)
```

### M√©thode 3 : Workflow de validation N8N

```
Items avec needs_review = true
    ‚Üì
Airtable (table de validation)
    ‚Üì
Humain valide et corrige
    ‚Üì
N8N Webhook (trigger sur update Airtable)
    ‚Üì
HTTP Request ‚Üí POST /mapping/add
    ‚Üì
Mapping ajout√© automatiquement
```

---

## üß™ Tests

### Test simple
```bash
# Tester l'unification
curl -X POST http://localhost:8002/unify \
  -H "Content-Type: application/json" \
  -d '{
    "text": "calcio",
    "type": "sport"
  }'
```

### Test batch
```bash
curl -X POST http://localhost:8002/unify/bulk \
  -H "Content-Type: application/json" \
  -d '{
    "items": [
      {"sport": "calcio", "tipText": "1X2: 1"},
      {"sport": "basket", "tipText": "over 2.5"}
    ]
  }'
```

---

## üìà Monitoring

### V√©rifier les stats
```bash
curl http://localhost:8002/health
```

### Voir tous les mappings sports
```bash
curl http://localhost:8002/mappings/sport
```

### Voir tous les mappings tip types
```bash
curl http://localhost:8002/mappings/tip_type
```

---

## üî• Troubleshooting

### Le service ne d√©marre pas
```bash
# V√©rifier Ollama
ollama list

# T√©l√©charger le mod√®le si absent
ollama pull nomic-embed-text

# V√©rifier les logs Docker
docker-compose logs -f
```

### Erreur "model not found"
```bash
# Entrer dans le container
docker exec -it unification-service bash

# T√©l√©charger le mod√®le
ollama pull nomic-embed-text
```

### ChromaDB vide apr√®s restart
- V√©rifier que le volume est bien mont√© : `./chroma_db:/app/chroma_db`
- Re-lancer `init_mappings.py`

---

## üìù Changelog

### v1.0.0
- ‚úÖ Unification sport et tip_type
- ‚úÖ API REST compl√®te
- ‚úÖ Support batch
- ‚úÖ ChromaDB persistant
- ‚úÖ Docker ready
- ‚úÖ Init script avec mappings de base

---

## üéØ Prochaines √©tapes

- [ ] Interface web pour g√©rer les mappings
- [ ] Authentification API
- [ ] M√©triques et analytics
- [ ] Support des comp√©titions
- [ ] Auto-learning avec feedback
